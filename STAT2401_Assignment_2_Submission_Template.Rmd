---
title: "Assignment 2 Submission"
author: "Ian Chen (24227644) and Marco Gunawan(23780778)"
date: "16/05/2025"
output:
  pdf_document: default
  fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment="")
```

# Statement of Contribution

Both people did both questions and discussed before submission.

    ```{r echo=FALSE,warning=FALSE}
    ## There are a few options in every chunk, click the setting button (greyed) near the triangle:
    # 1. to use default as in the above (show code and output, ie ECHO=TRUE) or 
    # 2. to show output only (ie ECHO=FALSE) or
    # 3. show nothing (run code) ie include=FALSE
    # 4. show nothing (dont run the code), like this chunk, ie eval=FALSE, include=FALSE
    ## You can knit this template into Word and then update it into pdf etc..
    ## You can create your own way for submission, this is just an example if you are going to use Rmd.
    
    suppressPackageStartupMessages({
          library(corrplot)
          library(ggplot2)
          library(gridExtra)
          library(purrr)
          library(knitr)
          library(dplyr)
          library(leaps)
        })
    ```

# Question 1: Air Pollution and Mortality. Does pollution kill people? (30 marks)

(a) (5 marks) Carry out exploratory data analysis (EDA) of this dataset.
    
    **Answer:**
    
    Climate EDA: The Mortality-Precip plot shows a strong postive linear relationship, indicating that as the precipitation increases, so does the Mortality. The Mortality-Humidity plot shows no clear relationship as datapoints appear randomly scattered. The Mortality-JanTemp plot depeicts either no clear relationship or a very weak negative relationship. Lastly the Mortality-JulyTemp plot depicts a moderate positive relationship with a few outliers. 
    
    Socioeconomic Variables EDA: From the pairs plot, we can see that the Mortality-Over65 graph appears to have no apparent relationship.The Mortality-House plot shows a weak to moderate positive relationship. The Mortality-Educ plot shows a strong negative relationship. The Mortality-Sound plot shows a moderate to strong negative relationship. The Mortality-Density plot has a Moderate positive relationship. The Mortality-NonWhite plot shows a strong positive relationship. The Mortality-WhiteCol plot shows no clear relationship. Lastly the Mortality-Poor plot shows a moderate positive relationship.
    
    Pollution Variables EDA: The Mortality-HC plot shows no correlation between the two variables, same with the Mortality-NOX plot.This is because the datapoints appear randomly scattered. However the Mortality SO2 plot depoicts a Moderate positive relationship. This means as the SO2 increases so does mortality.
    
    Mortality by Region Analysis: The south region has the highest rates of mortality, the highest median (975) as well as the widest range. The median and range are notably higher than then rest of the regions. The West region has the lowest average mortality rate (875) as well as the most narrow spread. The Midwest and Northeast are similar in average values (~940) and an outlier in the Midwest data. The regional differences stated suggests that the region a person lives in may influence their mortality rate.
    
    
    Mortality by State Code Analysis: Before we plotted out graph, I first examined the data, using table(pollution$State.code). Many of our states only have a singular datapoint and therefore are too small a sample size. Excluding the states with one datapoint we are left with 10 that we plot. The mean mortality rate differs greatly from each state. The states NY, PA, and OH have higher median mortality rates while CA and TX have significantly lower ones. States such as CA and PA having much wider interquartile range indicating a high variance. This suggests that the state an individual lives is relavent to their mortality rate.
    
    
    ```{r echo=FALSE,fig.height=3.75}
    pollution <- read.csv("Pollution.csv")
    
    #Climate EDA plot
    climate_variables <- pollution[, c("Mortality", "Precip", "Humidity", "JanTemp", "JulyTemp")]
    pairs(climate_variables, main = "Climate Variables vs Mortality")
    
    #Pollution EDA plot
    pollution_variables <- pollution[, c("Mortality", "HC", "NOX", "SO2")]
    pairs(pollution_variables, main = "Pollution Variables vs Mortality")
    
    
    #Combining the comparisons of the response against State.Code and Region respectively
    
    par(mfrow = c(1, 2))  # 1 row, 2 plots
    
    
    # Plot 1: Mortality vs filtered State.code
    table(pollution$State.code)
    filtered_pollution <- subset(pollution, State.code %in% c("CA", "CT", "MA", "MI", "MO", "NY", "OH", "PA", "TN", "TX"))
    boxplot(Mortality ~ State.code, data = filtered_pollution,
        main = "Mortality by State Code",
        xlab = "State Code", ylab = "Mortality",
        las = 2, col = "lightblue")
    
    # Plot 2: Mortality vs Region (unfiltered)
    boxplot(Mortality ~ Region, data = pollution,
        main = "Mortality by Region",
        xlab = "Region", ylab = "Mortality",
        las = 2, col = "lightgreen")
    
    ```
```{r echo=FALSE,fig.height=5}
    #Socioeconomic EDA plot
    socioeconomic_variables <- pollution[, c("Mortality", "Over65", "House", "Educ", "Sound", "Density", "NonWhite", "WhiteCol", "Poor")]
    pairs(socioeconomic_variables, main = "Socioeconomic Variables vs Mortality")
```
    
(b) (9 marks) Perform model selection process using the all subset method to arrive at good regression models that account for variation in mortality between the cities that can be attributed to differences in climate and socioeconomic factors. Identify optimal model(s) based on each of the adjusted R2 , BIC and $C_p$. Use a maximum of 10 variables for the selection. For each criterion, state the number and names of the selected variables.
    
    
    For the adjusted R^2 model, we have 7 variables. Those variables are Precip, JanTemp, JulyTemp, House, Educ, Density and NonWhite.
    For the BIC model, we have 4 variables. Those variables are JanTemp, House, Educ and NonWhite.
    For the Cp model, we have 6 variables. Those variables are Precip, JanTemp, JulyTemp, Educ, Density and NonWhite.
    
    
    ```{r echo=FALSE}
    library(leaps)
    source("all-subsets-lm.R")
    
    poll_data <- read.csv("Pollution.csv")
    
    #Using the subset of variables
    model <- lm(Mortality ~ Precip + Humidity + JanTemp + JulyTemp + 
                          Over65 + House + Educ + Sound + Density +
                          NonWhite + WhiteCol + Poor,
            data = poll_data)
    
    allsubsets_adjR2 <- all_subsets_lm(model, criterion = "adjR2",p_max = 10)
    allsubsets_bic   <- all_subsets_lm(model, criterion = "BIC",p_max = 10)
    allsubsets_c_p   <- all_subsets_lm(model, criterion = "Cp", p_max = 10)
    
    allsubsets_adjR2$variables
    allsubsets_bic$variables
    allsubsets_c_p$variables
    ```
    
    
(c) (4 marks) Fit the model with the lowest $C_p$ as obtained in (b). Write the equation of this fitted model. Interpret this model.
    
    **Answer:**
    
    Equation: Mortality = 1242.0+  1.401 * Precip -1.684 * JanTemp - 2.8 * JulyTemp -16.16 * Educ + 0.00757 * Density + 5.275 * Nonwhite
    
    Interpretation: From viewing the model summary, the R^2 value tells us that the model explains approximately 70.86% of the variation in the data's mortality rate. The model summary also tells us the F-Statistic and the p-value. Those being 21.48 and 1.305e-12 respectively. The large F-statistic (1<) suggests that the model has significant variables chosen. The small p-value (0.05>) also is evidence against the null hypothosies and that our model is statically significant. We then interpretthe coefficients for our model. A unit increase in Precip increases the rate by 1.401 deaths per 100,000. For each degree increase in JanTemp, mortality rate decreases by 1.684 deaths per 100,000. For each degree increase in JulyTemp, mortality rate decreases by 2.840 deaths per 100,000. Every year spend in education decreases the mortality rate by 16.16 per 100,000. Per unit increase in Density, the mortality rate increases by  0.00757 per 100,000. For every percentage increase in NonWhite, the mortality increases by 5.275 deaths per 100,000.
    
    
    ```{r echo=FALSE}
    poll_data <- read.csv("Pollution.csv")
    cp_model <- lm(Mortality ~ Precip + JanTemp + JulyTemp + Educ + Density + NonWhite, data = poll_data)
    
    summary(cp_model)
    ```
    
(d) (7 marks) Perform diagnostics checking on the model in (c). Do you think there are influential points in the data? Identify the cities which are influential points using leverage and Cook’s distance respectively.
    
    **Answer:**
    We first generate our diagnostic plots, checking the Residuals vs Fitted, Q-Q Residuals , Scale-Location and Residuals vs Leverage. The Residuals vs Fitted has a mostly random scatter of data points. There's minor curvature at the right end but overall this plot suggests the relationship is approximately linear. The Q-Q Residuals follows the diagonal line closely with minor deviations at the ends. Overall, the plot suggests that the residuals are approximately normally distributed.The Scale Location is fairly horizontal however towards the right end it appears to curve upwards. This suggests the variance may be slightly uneven. The Residuals vs Leverage plot depicts data points that require further investigation as they have a high leverage and are far from x=0. 
    
    Taking a further look at influential points using leverage and Cook’s distance, I use the equations 
    Leverage cutoff = (2(p+1))/ n and cook's cutoff  2 * (p + 1) / (n - p - 1) to find influential points. Using these equations as well as the linear model we can find the points above the cutoffs. The data points that are above the cook's cutoff is data point 20, the city being York. The data points above the leverage cutoff is data points 3,7,8,19 and 20. Their cities are San Diego, Miami, Los Angeles,San Francisco and York.
    
    
    ```{r,fig.height=3.65}
    poll_data <- read.csv("Pollution.csv")
    cp_model <- lm(Mortality ~ Precip + JanTemp + JulyTemp + Educ + 
                     Density + NonWhite, data = poll_data)
    # Model dimensions
    p <- length(coef(cp_model)) - 1
    n <- nrow(poll_data)
    #Diagnostics Plot
    par(mfrow = c(2, 2))
    plot(cp_model)
    ```
```{r,fig.height=3}
#The influential points are those with Cook's distance 
    par(mfrow = c(1, 1))
    cook_cutoff <- 2 * (p + 1) / (n - p - 1)
    plot(cp_model, which = 5, cook.levels = cook_cutoff)
    #Find Influential Points using Cook's Distance
    cooks_d <- cooks.distance(cp_model)
    cook_cutoff_points <- which(cooks_d > cook_cutoff)
    cat(cook_cutoff_points)
    #Find high leverage
    lev_cutoff <- 2 * (p + 1) / n
    high_leverage <- which(hatvalues(cp_model) > lev_cutoff)
    cat(high_leverage)
    
    #Find the Cities
    print(poll_data[cook_cutoff_points, "City"])
    print(poll_data[high_leverage, "City"])
```
    
    
(e) (5 marks) Using the model obtained in (c), add the three pollution variables (transformed to their natural logarithm) and obtain the p-value from the extrasum-of-squares F-test due to their addition. Summarise your findings in a few concise sentences.
    
    **Answer:**
    First I created the expanded model by adding the log-transformed variables of HC, NOX and SO2 into the base model.I used the extrasum-of-squares F-test on both the base and expanded linear models, giving me the p-value of 0.008313. This p-value is small, being less than 0.05. This means that the expanded model is statistically significant. This suggests the addition of the logarithm of the three pollution variables improved the model's ability to predict mortality rates. Therefore, we reject the null hypothosies. 
    
    
    ```{r}
    # Answer code here
    poll_data <- read.csv("Pollution.csv")
    cp_model <- lm(Mortality ~ Precip + JanTemp + JulyTemp + Educ 
                   + Density + NonWhite, 
                   data = poll_data)
    cp_model_expanded <- lm(Mortality ~ Precip + JanTemp + JulyTemp + Educ
                            + Density + NonWhite + log(HC) + log(NOX) +log(SO2), 
                            data = poll_data)
    anova(cp_model, cp_model_expanded)
    
    ```

# Question 2: Body Measurements (25 marks)

(a) (4 marks) Carry out exploratory data analysis (EDA) of this dataset before you do any modelling.

    ```{r include=FALSE}
    body <- read.csv("body.csv")
    names(body)
    ```

    Using str() and with the information provided with the task, we know that the dataset is mostly numeric, with 26 variables. With two variables standing out, primarily, X (the row index) and gender (a factor with two levels). I cleaned the data by removing X and performing EDA from then on.

    As there are 26 variables, and producing a pairs plot would result in 625 scatterplots, resulting in a plot which is too dense to view meaningfully, we have opted for a heatmap analysis instead.

    ```{r echo=FALSE,fig.height=3.8}
    body_clean <- body[,c(-1)]

    # Calculate correlation matrix
    cor_matrix <- cor(body_clean, use = "complete.obs")

    # Create correlation heatmap
    corrplot(cor_matrix, method = "color", type = "upper", 
             order = "hclust", tl.col = "black", tl.cex = 0.7) 
    ```

    Having a look at the heatmap itself, we can see that particularly weight, waist girth, chest girth and various other girth measurements appears to be strongly correlated.

    As gender is a categorical variable, I decided to create boxplots for further analysis with the numeric variables that are most strongly correlated to gender.

    For the main response variable, weight, we can see that male has a notably higher median weight (\~78kg) to female's median at (\~60kg). Both groups show some outliers at the upper end, particularly among females. These measurements show clear sexual dimorphism in human body proportions, with minimal overlap in distributions for measurements like height and waist girth.

(b) (10 marks) After the exploratory analysis has been carried out, construct two multiple linear regression models for this dataset using the training set.

    We begin by splitting the dataset into a training set and a testing set. Using both forward and backwards selection, we yield the following model equations.

    ```{r echo=FALSE, message=FALSE, warning=FALSE,fig.height=3}
    gender_body <- body_clean
    gender_body$Gender <- factor(gender_body$Gender, labels = c("Female", "Male"))

    create_boxplot <- function(var_name) {
      ggplot(gender_body, aes(x = factor(Gender), y = .data[[var_name]], fill = factor(Gender))) +
        geom_boxplot() +
        labs(title = paste(var_name, "by Gender"), x = "Gender", y = var_name) +
        theme_minimal() +
        theme(legend.position = "none")
    }

    gender_related_vars <- c("Weight", "Height", "Waist.girth", "Elbow.diameter")
    plots <- map(gender_related_vars, create_boxplot)

    grid.arrange(grobs = plots, ncol = 2)
    ```

    ```{r}
    # Data Selection
    set.seed(2401)
    test_index <- sample(nrow(body_clean), floor(0.2 * nrow(body_clean)))
    BodyMeasurementsTrain <- body_clean[-test_index, ]
    BodyMeasurementsTest <- body_clean[test_index, ]

    ```

    ```{r echo = FALSE}
    #Multiple Linear Regression Model
    set.seed(2401) #set seed for reproducibility
    body_lm <- lm(Weight ~ ., data = BodyMeasurementsTrain)
    body_lm_intercept_only <- lm(Weight ~ 1, data = BodyMeasurementsTrain)

    #Forward Selection
    body_forward <- step(
      body_lm_intercept_only, 
      scope = ~ (
        Biacromial.diameter + Biiliac.diameter + Bitrochanteric.diameter +
        Chest.depth + Chest.diameter + Elbow.diameter + Wrist.diameter +
        Knee.diameter + Ankle.diameter + Shoulder.girth + Chest.girth +
        Waist.girth + Navel.girth + Hip.girth + Thigh.girth + Bicep.girth +
        Forearm.girth + Knee.girth + Calf.maximum.girth + Ankle.minimum.girth +
        Wrist.minimum.girth + Age + Height + Gender
      ),
      direction = 'forward',
      trace = 0)


    #Backwards Selection
    body_backward <- step(
      body_lm,
      direction = 'backward',
      trace = 0
    )
    ```

    The first model given by the forward AIC step function (Model 1) is shown below:

    ```{r echo=FALSE}
    #further work on the forward selection model
    (summary(body_forward))
    ```

    And similarly, for the backward AIC step function (Model 2)

    ```{r echo=FALSE}
    #backward selection model
    (summary(body_backward))
    ```

    We notice that there are some variables tha tare a apart of this model that are not statistically significant, that is, their p-value is greater than 0.1. To yield two seperate models to be able to be compared however, we instead decided to remove all non-significant variables at once. And doing so sequentially, until all non-significant variables have been removed or until the overall model fit (adjusted R-squared, AIC, BIC) significantly worsen.

    ```{r include=FALSE}
    body_forward_large_1 <- lm(formula = Weight ~ Waist.girth + Height + Thigh.girth + Forearm.girth + 
    Shoulder.girth + Calf.maximum.girth + Hip.girth + Chest.girth + 
    Knee.diameter + Age + Chest.depth + Gender + Knee.girth + 
    Chest.diameter + Wrist.minimum.girth, data = BodyMeasurementsTrain)

    summary(body_forward_large_1)

    body_backwards_large_1 <- lm(formula = Weight ~ Chest.depth + Chest.diameter + Wrist.diameter + 
    Knee.diameter + Shoulder.girth + Chest.girth + Waist.girth + 
    Hip.girth + Thigh.girth + Forearm.girth + Knee.girth + 
    Calf.maximum.girth + Wrist.minimum.girth + Age + Height + 
    Gender, data = BodyMeasurementsTrain)

    summary(body_backwards_large_1)
    ```

    ```{r include=FALSE}
    #final model summary for forward selection
    body_forward_optimal <- lm(formula = Weight ~ Waist.girth + Height + Thigh.girth + Forearm.girth + 
    Shoulder.girth + Calf.maximum.girth + Hip.girth + Chest.girth + 
    Knee.diameter + Age + Chest.depth + Gender + Knee.girth, data = BodyMeasurementsTrain)

    summary(body_forward_optimal)

    #final model summary for backward selection
    body_backward_optimal<- lm(formula = Weight ~ Chest.depth + Wrist.diameter +  
                                 Knee.diameter + Shoulder.girth + Chest.girth + 
                                 Waist.girth + Hip.girth + Thigh.girth + Forearm.girth +
                                 Knee.girth + Calf.maximum.girth + Wrist.minimum.girth +
                                 Age + Height + Gender, 
                               data = BodyMeasurementsTrain)
    summary(body_backward_optimal)
    ```

    Thus, in the end we yield Model 1:
    
    Weight = -120.9683 + 0.3591 × Waist.girth + 0.3101 × Height + 0.2724 × Thigh.girth + 0.5641 × Forearm.girth + 0.0936 × Shoulder.girth + 0.3385 × Calf.maximum.girth + 0.2197 × Hip.girth + 0.1818 × Chest.girth + 0.5451 × Knee.diameter - 0.0562 × Age + 0.2679 × Chest.depth - 1.2184 × Gender + 0.1559 × Knee.girth

    We can see that all predictors in our model are statistically significant at the p \< 0.1 level, with the intercept being -121, which has no real meaning as someone could never have negative weight. Model 1's final adjusted R-squared value was 0.9744, with a F-statistic 1189 on 13 and 392 DF. Model 1's final variables includes Waist.girth, Height, Thigh.girth, Forearm.girth, Shoulder.girth, Calf.maximum.girth, Hip.girth, Chest.girth, Knee.diameter, Age, Chest.depth, Gender and Knee.girth.

    And similarly, for Model 2:
    
    Weight = -120.2155 + 0.2838 × Chest.depth + 0.4900 × Wrist.diameter + 0.5195 × Knee.diameter + 0.0963 × Shoulder.girth + 0.1767 × Chest.girth + 0.3566 × Waist.girth + 0.2193 × Hip.girth + 0.2631 × Thigh.girth + 0.6449 × Forearm.girth + 0.1701 × Knee.girth + 0.3488 × Calf.maximum.girth - 0.4509 × Wrist.minimum.girth - 0.0573 × Age + 0.3073 × Height - 1.2544 × Gender

  
    We can see that all predictors are statistically significant with the intercept being -120, which also has no real meaning. Model 2's final adjusted R-squared value was 0.9747, with a F-statistic 1041 on 15 and 390 DF. Model 2's final variables includes Chest.depth , Wrist.diameter , Knee.diameter , Shoulder.girth , Chest.girth , Waist.girth , Hip.girth , Thigh.girth , Forearm.girth , Knee.girth , Calf.maximum.girth , Wrist.minimum.girth , Age , Height , Gender.

(c) (5 marks) Perform diagnostics checking for each of the final fitted models, Model 1 and Model 2 respectively.

    For Model 1:

    ```{r echo=FALSE,fig.height=4}
    par(mfrow = c(2, 2))
    model_1 <- body_forward_optimal
    plot(model_1)
    ```

    The diagnostic plots indicate the the residuals are not mean zero. The Q-Q plot The points follow the diagonal line fairly well indicating the residuals are approximately normally distributed. The scale-location plot suggests the variance may also not be constant. Finally, no influential points are detected.

    For Model 2:

    ```{r echo = FALSE,fig.height=4}
    model_2 <- body_backward_optimal
    par(mfrow = c(2, 2))
    plot(model_2)
    ```

    The diagnostic plots indicate the the residuals are not mean zero though slighlty better than Model 1. The Q-Q plot The points follow the diagonal line fairly well indicating the residuals are approximately normally distributed. The scale-location plot suggests the variance may also not be constant. And no influential points are detected.

(d) (6 marks). Despite any inadequacies that you may or may not have identified above, you use the two models obtained in (b) to make predictions of Weight in the test set.

    (i) Produce a correctly drawn and labelled plot of predicted values against the actual values in the test set, and obtain the root mean squared error of prediction (RMSEP) based on each fitted model.

    ```{r include=FALSE}
    # We put the RMSPE calculation in a function
    rmspe <- function(pred, actual) {
      sqrt(mean((actual - pred)**2))
    }
    ```

    ```{r echo=FALSE,fig.height=2.75}
    model_1_pred <- predict(model_1, BodyMeasurementsTest)
    model_2_pred <- predict(model_2, BodyMeasurementsTest)

    prediction_data <- data.frame(
    ActualValue = BodyMeasurementsTest$Weight, # Assuming Weight is the target variable
    Model1_Prediction = model_1_pred,
    Model2_Prediction = model_2_pred
    )

    # Graph for Model 1
    plot1 <- ggplot(prediction_data, aes(x = ActualValue, y = Model1_Prediction)) +
    geom_point(alpha = 0.6, color = "darkblue") +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    labs(title = "Model 1 Predictions vs Actual",
         x = "Actual Values",
         y = "Predicted Values") +
    theme_minimal() +
    annotate("text", x = min(prediction_data$ActualValue) + 0.1 * diff(range(prediction_data$ActualValue)), y = max(prediction_data$Model1_Prediction) - 0.1 * diff(range(prediction_data$Model1_Prediction)), label = paste("        RMSE =", round(sqrt(mean((prediction_data$ActualValue - prediction_data$Model1_Prediction)^2)), 3)))

    # Graph for Model 2
    plot2 <- ggplot(prediction_data, aes(x = ActualValue, y = Model2_Prediction)) +
    geom_point(alpha = 0.6, color = "darkgreen") +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    labs(title = "Model 2 Predictions vs Actual", x = "Actual Values", y = "Predicted Values") + theme_minimal() + annotate("text", x = min(prediction_data$ActualValue) + 0.1 * diff(range(prediction_data$ActualValue)), y = max(prediction_data$Model2_Prediction) - 0.1 * diff(range(prediction_data$Model2_Prediction)), label = paste("        RMSE =", round(sqrt(mean((prediction_data$ActualValue - prediction_data$Model2_Prediction)^2)), 3)))

    # Arrange plots side by side
    grid.arrange(plot1, plot2, ncol = 2)
    ```

    (ii) Using the RMSEPs and the plots you produced, comment on how well the models performed.

    Both models appear to perform very similarly based on the visual plots and their RMSE (Root Mean Square Error of Prediction) values:

    -   Model 1 has an RMSE of 1.917
    -   Model 2 hsa a RMSE of 1.963

    Given the small difference in RMSE values and the similar visual patterns, both models appear to be performing well, with Model 1 having a marginally better overall performance based on the slightly lower RMSE.

    ```{r include=FALSE}
    #first removed <- elbow diameter
    body_forward_1 <- lm(Weight ~ Waist.girth + Height + Thigh.girth + Forearm.girth + 
    Shoulder.girth + Calf.maximum.girth + Hip.girth + Chest.girth + 
    Knee.diameter + Age + Chest.depth + Gender + Knee.girth + 
    Chest.diameter + Bicep.girth + Wrist.minimum.girth + 
    Wrist.diameter, 

    data = BodyMeasurementsTrain)

    (summary(body_forward_1))


    #first removed <- bicep girth
    body_backward_1 <- lm(formula = Weight ~ Chest.depth + Chest.diameter + 
                            Wrist.diameter + Knee.diameter + Shoulder.girth + 
                            Chest.girth + Waist.girth + Hip.girth + Thigh.girth + 
                            Forearm.girth + Knee.girth + Calf.maximum.girth + 
                            Wrist.minimum.girth + Age + Height + Gender, 
                          data = BodyMeasurementsTrain)
    summary(body_backward_1)
    ```

    ```{r include=FALSE}
    #second remove
    body_forward_2 <- lm(Weight ~ Waist.girth + Height + Thigh.girth + Forearm.girth + 
                        Shoulder.girth + Calf.maximum.girth + Hip.girth + Chest.girth + 
                          Knee.diameter + Age + Chest.depth + Gender + Knee.girth + 
                          Bicep.girth + Wrist.minimum.girth + 
                          Wrist.diameter, 
                        data = BodyMeasurementsTrain)

    summary(body_forward_2)

    #body backwards have reached 'optimal' solution
    body_backward_final <- lm(formula = Weight ~ Chest.depth + 
                            Wrist.diameter + Knee.diameter + Shoulder.girth + 
                            Chest.girth + Waist.girth + Hip.girth + Thigh.girth + 
                            Forearm.girth + Knee.girth + Calf.maximum.girth + 
                            Wrist.minimum.girth + Age + Height + Gender, 
                          data = BodyMeasurementsTrain)

    ```

    ```{r include=FALSE}
    #third removed
    body_forward_3 <- lm(Weight ~ Waist.girth + Height + Thigh.girth + Forearm.girth + 
                        Shoulder.girth + Calf.maximum.girth + Hip.girth + Chest.girth + 
                          Knee.diameter + Age + Chest.depth + Gender + Knee.girth + 
                          Chest.diameter + Wrist.minimum.girth + Wrist.diameter, 
                        data = BodyMeasurementsTrain)

    summary(body_forward_3)

    ```

    ```{r include=FALSE}
    #final remove of forwards
    body_forward_final <- lm(Weight ~ Waist.girth + Height + Thigh.girth + 
                                 Forearm.girth + Shoulder.girth + Calf.maximum.girth + 
                                 Hip.girth + Chest.girth + Knee.diameter + Age + 
                              Chest.depth + Gender + Knee.girth + Wrist.minimum.girth + 
                                Wrist.diameter, data = BodyMeasurementsTrain)

    summary(body_forward_final)
    summary(body_backward_final)
    ```

